{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sptg7D4U4iZP",
        "outputId": "d0fd4bb7-5147-433c-f12f-c1c7d2d39067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/my_drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/my_drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZjWW9sht9qC",
        "outputId": "25821310-ef91-4aa6-9bcf-0aaf6276e0d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May 27 13:56:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3UXHMoKm_OU",
        "outputId": "62430f08-ff8a-46f2-c2f8-cee4d619c5ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ],
      "source": [
        "%pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3o4tEiv7SO9"
      },
      "outputs": [],
      "source": [
        "'''Import Libraries'''\n",
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "import glob\n",
        "import cv2\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import os, glob, datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract, Reshape, Attention\n",
        "from keras.models import Model, load_model\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#import data_generator as dg\n",
        "import keras.backend as K\n",
        "import skimage\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.io import imread, imsave\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDLSkA8tUkG",
        "outputId": "ee629aa0-0c49-4078-8b3e-94f5fb2bca97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.experimental.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCt7QaeG7UYp"
      },
      "outputs": [],
      "source": [
        "'''Set Parameters'''\n",
        "## Params\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model', default='Deep_DeQuIP', type=str, help='choose a type of model')\n",
        "parser.add_argument('--batch_size', default=128, type=int, help='batch size')\n",
        "\n",
        "parser.add_argument('--train_data_clean', default='/content/my_drive/MyDrive/CNRS Research/data/400_CLEAN', type=str, help='path of train data clean')\n",
        "parser.add_argument('--train_data_noisy', default='/content/my_drive/MyDrive/CNRS Research/data/400_FOCUS', type=str, help='path of train data noisy')\n",
        "\n",
        "parser.add_argument('--kernel_size', default=5, type=int, help='Hamiltonian kernel size')\n",
        "parser.add_argument('--patches_size', default=50, type=int, help='patch size')\n",
        "\n",
        "parser.add_argument('--epoch', default=50, type=int, help='number of train epoches')\n",
        "parser.add_argument('--lr', default=1e-3, type=float, help='initial learning rate for Adam')\n",
        "parser.add_argument('--save_every', default=1, type=int, help='save model at every x epoches')\n",
        "parser.add_argument('-f', '--file', required=False)\n",
        "\n",
        "args = parser.parse_args()\n",
        "#args.save_every = args.epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmiWg1vT7Wdb"
      },
      "outputs": [],
      "source": [
        "'''Set Save Dir for Models'''\n",
        "save_dir = os.path.join('/content/my_drive/MyDrive/CNRS Research',\n",
        "                        args.model+'_NLResAttn_Memory')\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "  print(save_dir)\n",
        "  os.mkdir(save_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-FOVh737hir"
      },
      "outputs": [],
      "source": [
        "'''utility functions'''\n",
        "\n",
        "def findLastCheckpoint(save_dir):\n",
        "    file_list = glob.glob(os.path.join(save_dir,'model_*.hdf5'))  # returns names of all .hdf5 files\n",
        "    if file_list:\n",
        "        epochs_exist = []\n",
        "        for file in file_list:\n",
        "            result = re.findall(\".*model_(.*).hdf5.*\",file) # returns epoch number from the model checkpoint file\n",
        "            epochs_exist.append(int(result[0]))\n",
        "        initial_epoch=max(epochs_exist)\n",
        "    else:\n",
        "        initial_epoch = 0\n",
        "    return initial_epoch\n",
        "\n",
        "def log(*args,**kwargs):\n",
        "     print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:\"),*args,**kwargs)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = args.lr\n",
        "    if epoch<=20:\n",
        "        lr = initial_lr\n",
        "    elif epoch<=30:\n",
        "        lr = initial_lr/10\n",
        "    elif epoch<=40:\n",
        "        lr = initial_lr/20\n",
        "    else:\n",
        "        lr = initial_lr/20\n",
        "    log('current learning rate is %2.8f' %lr)\n",
        "    return lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EQ-hhQT8Xoq"
      },
      "outputs": [],
      "source": [
        "def train_datagen(epoch_iter=2000, epoch_num=5, batch_size=128, data_dir=args.train_data_noisy):\n",
        "  #Original Batch_size = 128\n",
        "  #Original iter = 2000\n",
        "  n_count = 0 # AneeshFix\n",
        "  while(True):\n",
        "      # n_count = 0 AneeshError\n",
        "      if n_count == 0:\n",
        "          #print(n_count)\n",
        "          #xs, ys = speckled_datagenerator(data_dir)  AneeshError?  # generate clean and noisy data\n",
        "          clean_data, noisy_data = speckled_datagenerator(data_dir) #AneeshFix\n",
        "\n",
        "          assert len(clean_data) % args.batch_size == 0, 'make sure the last iteration has a full batchsize, this is important if you use batch normalization!'\n",
        "\n",
        "          # normalize the pixel values between 0 and 1\n",
        "          clean_data = clean_data.astype('float32')/255.0\n",
        "          noisy_data = noisy_data.astype('float32')/255.0\n",
        "\n",
        "          indices = list(range(clean_data.shape[0]))\n",
        "          n_count = 1\n",
        "\n",
        "      for _ in range(epoch_num):\n",
        "          np.random.shuffle(indices)\n",
        "          for i in range(0, len(indices), batch_size):\n",
        "              clean_batch = clean_data[indices[i:i+batch_size]]\n",
        "              noisy_batch = noisy_data[indices[i:i+batch_size]]\n",
        "\n",
        "             # noise =  np.random.normal(0, args.sigma/255.0, batch_x.shape)\n",
        "\n",
        "              yield noisy_batch, clean_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHKYgBMZrLkd"
      },
      "outputs": [],
      "source": [
        "def make_batch(data):\n",
        "  data = np.array(data, dtype='uint8')\n",
        "  data = data.reshape((data.shape[0]*data.shape[1],data.shape[2],data.shape[3],1))\n",
        "  discard_n = len(data)-len(data) // batch_size*batch_size\n",
        "  data = np.delete(data, range(discard_n), axis = 0)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi_PY1bO7vhz"
      },
      "outputs": [],
      "source": [
        "def speckled_datagenerator(data_dir, verbose=False):\n",
        "\n",
        "    file_list = glob.glob(data_dir+'/*.png')  # returns names of all .png files in B_Mode Dir\n",
        "    #print('file_list', file_list)\n",
        "\n",
        "    data = []\n",
        "    data_clean = []\n",
        "\n",
        "    # generate patches for all images in the directory\n",
        "    for i in range(len(file_list)):\n",
        "        clean_patch, patch = gen_speckled_image_patches(file_list[i])\n",
        "\n",
        "        data.append(patch)\n",
        "        data_clean.append(clean_patch)\n",
        "\n",
        "        if verbose:\n",
        "            print('image :',str(i+1)+'/'+ str(len(file_list)))\n",
        "\n",
        "    # do for speckled data\n",
        "    data = np.array(data, dtype='uint8')\n",
        "    data = data.reshape((data.shape[0]*data.shape[1],data.shape[2],data.shape[3],1))\n",
        "    discard_n = len(data)-len(data)//batch_size*batch_size\n",
        "    data = np.delete(data,range(discard_n),axis = 0)\n",
        "\n",
        "    # do for clean data\n",
        "    data_clean = np.array(data_clean, dtype='uint8')\n",
        "    data_clean = data_clean.reshape((data_clean.shape[0]*data_clean.shape[1],data_clean.shape[2],data_clean.shape[3],1))\n",
        "    discard_n = len(data_clean)-len(data_clean)//batch_size*batch_size\n",
        "    data_clean = np.delete(data_clean,range(discard_n),axis = 0)\n",
        "\n",
        "    print('-----training data finished-----')\n",
        "    print('noisy image shape:',data.shape)\n",
        "    print('clean image shape:',data_clean.shape)\n",
        "\n",
        "    assert data.shape == data_clean.shape\n",
        "\n",
        "\n",
        "    return data_clean, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ktmhpD-8xur"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def gen_speckled_image_patches(file_name):\n",
        "\n",
        "    last_name = file_name.split('_')[-1] #  Returns Name of the Image\n",
        "    clean_image_file_name = os.path.join(args.train_data_clean, last_name) # clean train image directory\n",
        "\n",
        "    img = cv2.imread(file_name, 0) # noisy image\n",
        "    clean_img = cv2.imread(clean_image_file_name, 0) # clean image\n",
        "\n",
        "    '''show(np.hstack((clean_img,img))) # display the images'''\n",
        "\n",
        "    h, w = img.shape\n",
        "\n",
        "    patches = []\n",
        "    clean_patches = []\n",
        "\n",
        "    for s in scales: # scaling the images\n",
        "        h_scaled, w_scaled = int(h*s),int(w*s)\n",
        "        img_scaled = cv2.resize(img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
        "        clean_img_scaled = cv2.resize(clean_img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # extract patches from the images '''ANEESH: PATCH 1 GETTING GENERATED'''\n",
        "        for i in range(0, h_scaled-patch_size+1, stride):\n",
        "            for j in range(0, w_scaled-patch_size+1, stride):\n",
        "                patch = img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "                clean_patch = clean_img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "\n",
        "                # data augmentation\n",
        "                for k in range(0, aug_times):\n",
        "                  mode_k=np.random.randint(0,8)\n",
        "                  patch_aug = data_augmentation(patch, mode=mode_k)\n",
        "                  clean_patch_aug = data_augmentation(clean_patch, mode=mode_k)\n",
        "                  patches.append(patch_aug)\n",
        "                  clean_patches.append(clean_patch_aug)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return clean_patches, patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAKj62Vp8bXU"
      },
      "outputs": [],
      "source": [
        "'''Data Augmentation'''\n",
        "def data_augmentation(img, mode=0):\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return np.flipud(img)\n",
        "    elif mode == 2:\n",
        "        return np.rot90(img)\n",
        "    elif mode == 3:\n",
        "        return np.flipud(np.rot90(img))\n",
        "    elif mode == 4:\n",
        "        return np.rot90(img, k=2)\n",
        "    elif mode == 5:\n",
        "        return np.flipud(np.rot90(img, k=2))\n",
        "    elif mode == 6:\n",
        "        return np.rot90(img, k=3)\n",
        "    elif mode == 7:\n",
        "        return np.flipud(np.rot90(img, k=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHhB2Ggv70Fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''Show Images'''\n",
        "def show(x,title=None,cbar=False,figsize=None):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(x,interpolation='nearest',cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if cbar:\n",
        "        plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCsHTWNK78rr"
      },
      "outputs": [],
      "source": [
        "'''Loss Function'''\n",
        "def sum_squared_error(y_true, y_pred):\n",
        "    #return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "    #return K.sum(K.square(y_pred - y_true), axis=-1)/2\n",
        "    return K.sum(K.square(y_pred - y_true))/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OPm8yDOKNlV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, activations, initializers\n",
        "\n",
        "class NonLocalResAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, use_bias=True, bn=True, act=True, res_scale=1):\n",
        "        super(NonLocalResAttentionBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.use_bias = use_bias\n",
        "        self.bn = bn\n",
        "        self.act = act\n",
        "        self.multiheadattn = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=out_channels)\n",
        "        self.res_scale = res_scale\n",
        "        self.conv1 = Conv2D(out_channels, kernel_size, padding='same', use_bias=use_bias)\n",
        "        self.bn1 = BatchNormalization() if bn else None\n",
        "        self.act1 = Activation('relu') if act else None\n",
        "        self.nl = NonLocalBlock2D(out_channels)\n",
        "        self.conv2 = Conv2D(out_channels, kernel_size, padding='same', use_bias=use_bias)\n",
        "        self.bn2 = BatchNormalization() if bn else None\n",
        "        self.act2 = Activation('relu') if act else None\n",
        "\n",
        "    def call(self, x):\n",
        "        x1 = self.multiheadattn(x, x)\n",
        "        res = self.conv1(x1)\n",
        "        if self.bn1:\n",
        "            res = self.bn1(res)\n",
        "        if self.act1:\n",
        "            res = self.act1(res)\n",
        "        nl = self.nl(res)\n",
        "        res = self.conv2(nl)\n",
        "        if self.bn2:\n",
        "            res = self.bn2(res)\n",
        "        if self.act2:\n",
        "            res = self.act2(res)\n",
        "        return x + res * self.res_scale\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'in_channels': self.in_channels,\n",
        "            'out_channels': self.out_channels,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'use_bias': self.use_bias,\n",
        "            'bn': self.bn,\n",
        "            'act': self.act,\n",
        "            'res_scale': self.res_scale\n",
        "        }\n",
        "        base_config = super(NonLocalResAttentionBlock, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, use_bias=True):\n",
        "    return layers.Conv2D(\n",
        "        filters=out_channels, kernel_size=kernel_size, padding='same', use_bias=use_bias\n",
        "    )\n",
        "\n",
        "\n",
        "class NonLocalBlock2D(tf.keras.layers.Layer): # long-range dependency\n",
        "    def __init__(self, in_channels, inter_channels=None, use_bias=True):\n",
        "        super(NonLocalBlock2D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = inter_channels if inter_channels else in_channels // 2\n",
        "        self.g = Conv2D(self.inter_channels, (1, 1), use_bias=use_bias)\n",
        "        self.theta = Conv2D(self.inter_channels, (1, 1), use_bias=use_bias)\n",
        "        self.phi = Conv2D(self.inter_channels, (1, 1), use_bias=use_bias)\n",
        "        self.W = Conv2D(self.in_channels, (1, 1), use_bias=use_bias)\n",
        "\n",
        "    def call(self, x):\n",
        "        batch_size, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
        "        g_x = tf.reshape(self.g(x), (batch_size, -1, self.inter_channels))\n",
        "        theta_x = tf.reshape(self.theta(x), (batch_size, -1, self.inter_channels))\n",
        "        phi_x = tf.reshape(self.phi(x), (batch_size, -1, self.inter_channels))\n",
        "\n",
        "        theta_phi = tf.matmul(theta_x, phi_x, transpose_b=True)\n",
        "        theta_phi = tf.nn.softmax(theta_phi)\n",
        "\n",
        "        y = tf.matmul(theta_phi, g_x)\n",
        "        y = tf.reshape(y, (batch_size, h, w, self.inter_channels))\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJOxFp6R76eh"
      },
      "outputs": [],
      "source": [
        "'''DIVA2D Model'''\n",
        "def DIVA2D(depth,filters=64,image_channels=1, kernel_size=5, use_bnorm=True):\n",
        "    layer_count = 0\n",
        "    inpt = Input(shape=(None,None,image_channels),name = 'input'+str(layer_count))\n",
        "\n",
        "    # Get the initial patches /initial_patches '''ANEESH: SMALLER PATCHES GENERATED FROM BIGGER PATCHES BY GENERATE_PATCH FUNCTION, IMAGE DIMENSION REMAINS SAME'''\n",
        "    initial_patches = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',name = 'initial_patches')(inpt)\n",
        "    initial_patches = Activation('relu',name = 'initial_patch_acti')(initial_patches)\n",
        "    #print('initial shape', initial_patches.get_shape())\n",
        "\n",
        "\n",
        "    inter = NonLocalResAttentionBlock(conv, filters, kernel_size, use_bias=True, bn=False, act=True, res_scale=1)(initial_patches)\n",
        "    inter = Activation('relu',name = 'inter_acti')(inter)\n",
        "\n",
        "    #print('inter shape',inter.get_shape())\n",
        "\n",
        "    # Get contributions of the original potential in the Hamiltonian kernel ANEESH: Ja from DIVA Diagram\n",
        "    ori_poten_kernel = tf.keras.layers.MaxPooling2D (pool_size=(21,21), strides=(15,15), padding='same', name = 'ori_poten_ker', data_format=None )(initial_patches)\n",
        "    #print('ori_poten_kernel',ori_poten_kernel.get_shape())\n",
        "\n",
        "    # Get contributions of the interactions in the Hamiltonian kernel ANEESH: Ia from DIVA Diagram\n",
        "    inter_kernel = tf.keras.layers.MaxPooling2D (pool_size=(21,21), strides=(15,15), padding='same', name = 'inter_ker', data_format=None )(inter)\n",
        "    #print('inter_kernel',inter_kernel.get_shape())\n",
        "\n",
        "\n",
        "    # Get projection coefficients of the initial patches on the Hamiltonian kernel\n",
        "    x = Hamiltonian_Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), kernel_3 = ori_poten_kernel, kernel_4 = inter_kernel, strides=(1,1), activation='relu',\n",
        "                              kernel_initializer='Orthogonal', padding='same', name = 'proj_coef')(inter)\n",
        "\n",
        "    #print('coef',x.get_shape())\n",
        "\n",
        "\n",
        "    # Do Thresholding (depth depends on the noise intensity)\n",
        "    for i in range(depth):\n",
        "      layer_count += 1\n",
        "      x = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "\n",
        "      layer_count += 1\n",
        "      x = BatchNormalization(axis=3, momentum=0.1,epsilon=0.0001, name = 'bn'+str(layer_count))(x)\n",
        "        #x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.0001, name = 'bn'+str(layer_count))(x)\n",
        "\n",
        "      # Thresholding\n",
        "      x = Activation('relu',name = 'Thresholding'+str(layer_count))(x)\n",
        "\n",
        "    # Inverse projection\n",
        "    x = Conv2D(filters=image_channels, kernel_size=(kernel_size,kernel_size), strides=(1,1), kernel_initializer='Orthogonal',padding='same',use_bias = False,name = 'inv_trans')(x)\n",
        "\n",
        "\n",
        "    # Deconvolution layer ANEESH: NEUTRALIZATION LAYER SIMILAR TO DIVA-A?\n",
        "    layer_count += 1\n",
        "    x = Conv2D(filters=filters, kernel_size=(args.kernel_size,args.kernel_size), strides=(1,1), kernel_initializer='Orthogonal',padding='same',use_bias = False,name = 'deconv'+str(layer_count))(x)\n",
        "    layer_count += 1\n",
        "    x = Conv2D(filters=filters, kernel_size=(args.kernel_size,args.kernel_size), strides=(1,1), kernel_initializer='Orthogonal',padding='same',use_bias = False,name = 'deconv'+str(layer_count))(x)\n",
        "    layer_count += 1\n",
        "    x = Conv2D(filters=image_channels, kernel_size=(args.kernel_size,args.kernel_size), strides=(1,1), kernel_initializer='Orthogonal',padding='same',use_bias = False,name = 'deconv'+str(layer_count))(x)\n",
        "    layer_count += 1\n",
        "\n",
        "\n",
        "    x = Subtract(name = 'subtract')([inpt, x])   # input - noise '''ANEESH: Noisy Image is getting subtracted from the Denoised Image'''\n",
        "\n",
        "    model = Model(inputs=inpt, outputs=x)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Ajlpp5ueWM"
      },
      "outputs": [],
      "source": [
        " '''Hamiltonian convolution layer'''\n",
        "class Hamiltonian_Conv2D(Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, kernel_3=None, kernel_4=None, activation=None, use_bias = False, **kwargs):\n",
        "\n",
        "        self.rank = 2               # Dimension of the kernel\n",
        "        self.num_filters = filters  # Number of filter in the convolution layer\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank, 'kernel_size')\n",
        "        self.kernel_3 = kernel_3    # Weights from original potential\n",
        "        self.kernel_4 = kernel_4    # Weights from interaction\n",
        "\n",
        "        super(Hamiltonian_Conv2D, self).__init__(self.num_filters, self.kernel_size,\n",
        "              activation=activation, use_bias=False, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                     'should be defined. Found `None`.')\n",
        "\n",
        "        #don't use bias:\n",
        "        self.bias = None\n",
        "\n",
        "        #consider the layer built\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "        # Define nabla operator\n",
        "        weights_1 = tf.constant([[ 2.,-1., 0.],\n",
        "                                 [-1., 4.,-1.],\n",
        "                                 [ 0.,-1., 2.]])\n",
        "\n",
        "\n",
        "        weights_1 = tf.reshape(weights_1 , [3,3, 1])\n",
        "        weights_1 = tf.repeat(weights_1 , repeats=self.num_filters, axis=2)\n",
        "        #print('kernel shape of weights_1:',weights_1.get_shape())\n",
        "\n",
        "        # Define Weights for h^2/2m  (size should be same as the nabla operator)\n",
        "        weights_2 = self.add_weight(shape=weights_1.get_shape(),\n",
        "                                      initializer= 'Orthogonal',\n",
        "                                      name='kernel_h^2/2m',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        #print('kernel shape of weights_2:',weights_2.get_shape())\n",
        "        #reshaped_weights = tf.expand_dims(weights_1*weights_2, axis=0)\n",
        "\n",
        "\n",
        "        # Define the Hamiltonian kernel\n",
        "        self.kernel = weights_1*weights_2 + self.kernel_3 + self.kernel_4\n",
        "        #print('self.kernel',self.kernel.get_shape())\n",
        "\n",
        "        self.built = True\n",
        "        super(Hamiltonian_Conv2D, self).build(input_shape)\n",
        "\n",
        "    # Do the 2D convolution using the Hamiltonian kernel\n",
        "    def convolution_op(self, inputs, kernel):\n",
        "        if self.padding == \"causal\":\n",
        "            tf_padding = \"VALID\"  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, str):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "\n",
        "\n",
        "        return tf.nn.convolution(\n",
        "            inputs,\n",
        "            kernel,\n",
        "            strides=list(self.strides),\n",
        "            padding=tf_padding,\n",
        "            dilations=list(self.dilation_rate),\n",
        "            name=self.__class__.__name__,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.convolution_op(inputs, self.kernel)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz0OteFv6AoF"
      },
      "outputs": [],
      "source": [
        "model = DIVA2D(depth=15,filters=96,image_channels=1,use_bnorm=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNIHpeSg6KEH",
        "outputId": "53aa52aa-c39c-4ae7-f2fa-6fb6b6605b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input0 (InputLayer)         [(None, None, None, 1)]      0         []                            \n",
            "                                                                                                  \n",
            " initial_patches (Conv2D)    (None, None, None, 96)       2496      ['input0[0][0]']              \n",
            "                                                                                                  \n",
            " initial_patch_acti (Activa  (None, None, None, 96)       0         ['initial_patches[0][0]']     \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " non_local_res_attention_bl  (None, None, None, 96)       516912    ['initial_patch_acti[0][0]']  \n",
            " ock (NonLocalResAttentionB                                                                       \n",
            " lock)                                                                                            \n",
            "                                                                                                  \n",
            " inter_acti (Activation)     (None, None, None, 96)       0         ['non_local_res_attention_bloc\n",
            "                                                                    k[0][0]']                     \n",
            "                                                                                                  \n",
            " proj_coef (Hamiltonian_Con  (None, None, None, 96)       231264    ['inter_acti[0][0]']          \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, None, None, 96)       230400    ['proj_coef[0][0]']           \n",
            "                                                                                                  \n",
            " bn2 (BatchNormalization)    (None, None, None, 96)       384       ['conv1[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding2 (Activation)  (None, None, None, 96)       0         ['bn2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding2[0][0]']       \n",
            "                                                                                                  \n",
            " bn4 (BatchNormalization)    (None, None, None, 96)       384       ['conv3[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding4 (Activation)  (None, None, None, 96)       0         ['bn4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv5 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding4[0][0]']       \n",
            "                                                                                                  \n",
            " bn6 (BatchNormalization)    (None, None, None, 96)       384       ['conv5[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding6 (Activation)  (None, None, None, 96)       0         ['bn6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv7 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding6[0][0]']       \n",
            "                                                                                                  \n",
            " bn8 (BatchNormalization)    (None, None, None, 96)       384       ['conv7[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding8 (Activation)  (None, None, None, 96)       0         ['bn8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv9 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding8[0][0]']       \n",
            "                                                                                                  \n",
            " bn10 (BatchNormalization)   (None, None, None, 96)       384       ['conv9[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding10 (Activation  (None, None, None, 96)       0         ['bn10[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv11 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding10[0][0]']      \n",
            "                                                                                                  \n",
            " bn12 (BatchNormalization)   (None, None, None, 96)       384       ['conv11[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding12 (Activation  (None, None, None, 96)       0         ['bn12[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv13 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding12[0][0]']      \n",
            "                                                                                                  \n",
            " bn14 (BatchNormalization)   (None, None, None, 96)       384       ['conv13[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding14 (Activation  (None, None, None, 96)       0         ['bn14[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv15 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding14[0][0]']      \n",
            "                                                                                                  \n",
            " bn16 (BatchNormalization)   (None, None, None, 96)       384       ['conv15[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding16 (Activation  (None, None, None, 96)       0         ['bn16[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv17 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding16[0][0]']      \n",
            "                                                                                                  \n",
            " bn18 (BatchNormalization)   (None, None, None, 96)       384       ['conv17[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding18 (Activation  (None, None, None, 96)       0         ['bn18[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv19 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding18[0][0]']      \n",
            "                                                                                                  \n",
            " bn20 (BatchNormalization)   (None, None, None, 96)       384       ['conv19[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding20 (Activation  (None, None, None, 96)       0         ['bn20[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv21 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding20[0][0]']      \n",
            "                                                                                                  \n",
            " bn22 (BatchNormalization)   (None, None, None, 96)       384       ['conv21[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding22 (Activation  (None, None, None, 96)       0         ['bn22[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv23 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding22[0][0]']      \n",
            "                                                                                                  \n",
            " bn24 (BatchNormalization)   (None, None, None, 96)       384       ['conv23[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding24 (Activation  (None, None, None, 96)       0         ['bn24[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv25 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding24[0][0]']      \n",
            "                                                                                                  \n",
            " bn26 (BatchNormalization)   (None, None, None, 96)       384       ['conv25[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding26 (Activation  (None, None, None, 96)       0         ['bn26[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv27 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding26[0][0]']      \n",
            "                                                                                                  \n",
            " bn28 (BatchNormalization)   (None, None, None, 96)       384       ['conv27[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding28 (Activation  (None, None, None, 96)       0         ['bn28[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv29 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding28[0][0]']      \n",
            "                                                                                                  \n",
            " bn30 (BatchNormalization)   (None, None, None, 96)       384       ['conv29[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding30 (Activation  (None, None, None, 96)       0         ['bn30[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " inv_trans (Conv2D)          (None, None, None, 1)        2400      ['Thresholding30[0][0]']      \n",
            "                                                                                                  \n",
            " deconv31 (Conv2D)           (None, None, None, 96)       2400      ['inv_trans[0][0]']           \n",
            "                                                                                                  \n",
            " deconv32 (Conv2D)           (None, None, None, 96)       230400    ['deconv31[0][0]']            \n",
            "                                                                                                  \n",
            " deconv33 (Conv2D)           (None, None, None, 1)        2400      ['deconv32[0][0]']            \n",
            "                                                                                                  \n",
            " subtract (Subtract)         (None, None, None, 1)        0         ['input0[0][0]',              \n",
            "                                                                     'deconv33[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4450032 (16.98 MB)\n",
            "Trainable params: 4447152 (16.96 MB)\n",
            "Non-trainable params: 2880 (11.25 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5k_OuLm7o6H"
      },
      "outputs": [],
      "source": [
        "'''Hyperparameters'''\n",
        "patch_size, stride = 25, 10\n",
        "aug_times = 1\n",
        "scales = [1] # [1, 0.9, 0.8, 0.7]\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxqNGvHd7-zY"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # model selection\n",
        "    model = DIVA2D(depth=15,filters=96,image_channels=1,use_bnorm=True)\n",
        "    #model.summary()\n",
        "\n",
        "    # load the last model in matconvnet style\n",
        "    initial_epoch = findLastCheckpoint(save_dir=save_dir)\n",
        "    if initial_epoch > 0:\n",
        "        print('resuming by loading epoch %03d'%initial_epoch)\n",
        "        model.load_weights(os.path.join(save_dir,'model_%03d.hdf5'%initial_epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=Adam(0.001), loss= tf.keras.losses.MeanSquaredError(), #tf.keras.losses.CosineSimilarity (axis=-1, reduction=\"auto\", name=\"cosine_similarity\"),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(),\n",
        "                           tf.keras.metrics.RootMeanSquaredError(),\n",
        "                           tf.keras.metrics.MeanSquaredLogarithmicError(),\n",
        "                           tf.keras.metrics.MeanAbsoluteError(),\n",
        "                           sum_squared_error])\n",
        "\n",
        "    # tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.CosineSimilarity(name=\"cosine_similarity\", dtype=None, axis=-1),\n",
        "    # tf.keras.metrics.LogCoshError(),\n",
        "\n",
        "    # use call back functions\n",
        "    checkpointer = ModelCheckpoint(os.path.join(save_dir,'model_{epoch:03d}.hdf5'),\n",
        "                verbose=1, save_weights_only=False, period=1)\n",
        "    csv_logger = CSVLogger(os.path.join(save_dir,'log.csv'), append=True, separator=',')\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    print('batch_size = ',args.batch_size)\n",
        "    history = model.fit(train_datagen(batch_size=args.batch_size),\n",
        "                steps_per_epoch=4750, epochs=80, verbose=1, initial_epoch=initial_epoch,\n",
        "                callbacks=[checkpointer,csv_logger,lr_scheduler])\n",
        "    #steps_per_epoch = 7000, epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PnoCdQIgIqo"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
