{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZjWW9sht9qC",
        "outputId": "9ff4c280-7817-4d92-d1dc-c3b7173fc417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 19 14:46:44 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3UXHMoKm_OU",
        "outputId": "4b4234c4-0d90-430b-cdc3-7f0198c594b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('myDrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm_YbEHnhTbY",
        "outputId": "ab925815-2fa2-45d7-da87-7723c4cbe938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at myDrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Import Libraries'''\n",
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "import glob\n",
        "import cv2\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import os, glob, datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract, Reshape, MultiHeadAttention, MaxPooling2D,LayerNormalization\n",
        "from keras.models import Model, load_model\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#import data_generator as dg\n",
        "import keras.backend as K\n",
        "import skimage\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.io import imread, imsave\n"
      ],
      "metadata": {
        "id": "E3o4tEiv7SO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDLSkA8tUkG",
        "outputId": "19b77529-21cf-4001-9c11-1a85c26dc0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Set Parameters'''\n",
        "## Params\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model', default='Deep_DeQuIP', type=str, help='choose a type of model')\n",
        "parser.add_argument('--batch_size', default=128, type=int, help='batch size')\n",
        "\n",
        "parser.add_argument('--train_data_clean', default='/content/myDrive/MyDrive/CNRS Research/data/400_CLEAN', type=str, help='path of train data clean')\n",
        "parser.add_argument('--train_data_noisy', default='/content/myDrive/MyDrive/CNRS Research/data/400_FOCUS', type=str, help='path of train data noisy')\n",
        "\n",
        "parser.add_argument('--kernel_size', default=5, type=int, help='Hamiltonian kernel size')\n",
        "parser.add_argument('--patches_size', default=50, type=int, help='patch size')\n",
        "\n",
        "parser.add_argument('--epoch', default=50, type=int, help='number of train epoches')\n",
        "parser.add_argument('--lr', default=1e-3, type=float, help='initial learning rate for Adam')\n",
        "parser.add_argument('--save_every', default=1, type=int, help='save model at every x epoches')\n",
        "parser.add_argument('-f', '--file', required=False)\n",
        "\n",
        "args = parser.parse_args()\n",
        "#args.save_every = args.epoch\n"
      ],
      "metadata": {
        "id": "XCt7QaeG7UYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Set Save Dir for Models'''\n",
        "save_dir = os.path.join('/content/myDrive/MyDrive/CNRS Research',\n",
        "                        args.model+'_MultiHeadAttn_Memory')\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "  print(save_dir)\n",
        "  os.mkdir(save_dir)\n"
      ],
      "metadata": {
        "id": "NmiWg1vT7Wdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''utility functions'''\n",
        "\n",
        "def findLastCheckpoint(save_dir):\n",
        "    file_list = glob.glob(os.path.join(save_dir,'model_*.hdf5'))  # returns names of all .hdf5 files\n",
        "    if file_list:\n",
        "        epochs_exist = []\n",
        "        for file in file_list:\n",
        "            result = re.findall(\".*model_(.*).hdf5.*\",file) # returns epoch number from the model checkpoint file\n",
        "            epochs_exist.append(int(result[0]))\n",
        "        initial_epoch=max(epochs_exist)\n",
        "    else:\n",
        "        initial_epoch = 0\n",
        "    return initial_epoch\n",
        "\n",
        "def log(*args,**kwargs):\n",
        "     print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:\"),*args,**kwargs)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = args.lr\n",
        "    if epoch<=20:\n",
        "        lr = initial_lr\n",
        "    elif epoch<=30:\n",
        "        lr = initial_lr/10\n",
        "    elif epoch<=40:\n",
        "        lr = initial_lr/20\n",
        "    else:\n",
        "        lr = initial_lr/20\n",
        "    log('current learning rate is %2.8f' %lr)\n",
        "    return lr\n"
      ],
      "metadata": {
        "id": "Y-FOVh737hir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_datagen(epoch_iter=2000, epoch_num=5, batch_size=128, data_dir=args.train_data_noisy):\n",
        "  #Original Batch_size = 128\n",
        "  #Original iter = 2000\n",
        "  n_count = 0 # AneeshFix\n",
        "  while(True):\n",
        "      # n_count = 0 AneeshError\n",
        "      if n_count == 0:\n",
        "          #print(n_count)\n",
        "          #xs, ys = speckled_datagenerator(data_dir)  AneeshError?  # generate clean and noisy data\n",
        "          clean_data, noisy_data = speckled_datagenerator(data_dir) #AneeshFix\n",
        "\n",
        "          assert len(clean_data) % args.batch_size == 0, 'make sure the last iteration has a full batchsize, this is important if you use batch normalization!'\n",
        "\n",
        "          # normalize the pixel values between 0 and 1\n",
        "          clean_data = clean_data.astype('float32')/255.0\n",
        "          noisy_data = noisy_data.astype('float32')/255.0\n",
        "\n",
        "          indices = list(range(clean_data.shape[0]))\n",
        "          n_count = 1\n",
        "\n",
        "      for _ in range(epoch_num):\n",
        "          np.random.shuffle(indices)\n",
        "          for i in range(0, len(indices), batch_size):\n",
        "              clean_batch = clean_data[indices[i:i+batch_size]]\n",
        "              noisy_batch = noisy_data[indices[i:i+batch_size]]\n",
        "\n",
        "             # noise =  np.random.normal(0, args.sigma/255.0, batch_x.shape)\n",
        "\n",
        "              yield noisy_batch, clean_batch"
      ],
      "metadata": {
        "id": "7EQ-hhQT8Xoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(data):\n",
        "  data = np.array(data, dtype='uint8')\n",
        "  data = data.reshape((data.shape[0]*data.shape[1],data.shape[2],data.shape[3],1))\n",
        "  discard_n = len(data)-len(data) // batch_size*batch_size\n",
        "  data = np.delete(data, range(discard_n), axis = 0)\n",
        "  return data"
      ],
      "metadata": {
        "id": "bHKYgBMZrLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speckled_datagenerator(data_dir, verbose=False):\n",
        "\n",
        "    file_list = glob.glob(data_dir+'/*.png')  # returns names of all .png files in B_Mode Dir\n",
        "    #print('file_list', file_list)\n",
        "\n",
        "    data = []\n",
        "    data_clean = []\n",
        "\n",
        "    # generate patches for all images in the directory\n",
        "    for i in range(len(file_list)):\n",
        "        clean_patch, patch = gen_speckled_image_patches(file_list[i])\n",
        "\n",
        "        data.append(patch)\n",
        "        data_clean.append(clean_patch)\n",
        "\n",
        "        if verbose:\n",
        "            print('image :',str(i+1)+'/'+ str(len(file_list)))\n",
        "\n",
        "    # do for speckled data\n",
        "    data = np.array(data, dtype='uint8')\n",
        "    data = data.reshape((data.shape[0]*data.shape[1],data.shape[2],data.shape[3],1))\n",
        "    discard_n = len(data)-len(data)//batch_size*batch_size\n",
        "    data = np.delete(data,range(discard_n),axis = 0)\n",
        "\n",
        "    # do for clean data\n",
        "    data_clean = np.array(data_clean, dtype='uint8')\n",
        "    data_clean = data_clean.reshape((data_clean.shape[0]*data_clean.shape[1],data_clean.shape[2],data_clean.shape[3],1))\n",
        "    discard_n = len(data_clean)-len(data_clean)//batch_size*batch_size\n",
        "    data_clean = np.delete(data_clean,range(discard_n),axis = 0)\n",
        "\n",
        "    print('-----training data finished-----')\n",
        "    print('noisy image shape:',data.shape)\n",
        "    print('clean image shape:',data_clean.shape)\n",
        "\n",
        "    assert data.shape == data_clean.shape\n",
        "\n",
        "\n",
        "    return data_clean, data"
      ],
      "metadata": {
        "id": "yi_PY1bO7vhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def gen_speckled_image_patches(file_name):\n",
        "\n",
        "    last_name = file_name.split('_')[-1] #  Returns Name of the Image\n",
        "    clean_image_file_name = os.path.join(args.train_data_clean, last_name) # clean train image directory\n",
        "\n",
        "    img = cv2.imread(file_name, 0) # noisy image\n",
        "    clean_img = cv2.imread(clean_image_file_name, 0) # clean image\n",
        "\n",
        "    '''show(np.hstack((clean_img,img))) # display the images'''\n",
        "\n",
        "    h, w = img.shape\n",
        "\n",
        "    patches = []\n",
        "    clean_patches = []\n",
        "\n",
        "    for s in scales: # scaling the images\n",
        "        h_scaled, w_scaled = int(h*s),int(w*s)\n",
        "        img_scaled = cv2.resize(img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
        "        clean_img_scaled = cv2.resize(clean_img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # extract patches from the images '''ANEESH: PATCH 1 GETTING GENERATED'''\n",
        "        for i in range(0, h_scaled-patch_size+1, stride):\n",
        "            for j in range(0, w_scaled-patch_size+1, stride):\n",
        "                patch = img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "                clean_patch = clean_img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "\n",
        "                # data augmentation\n",
        "                for k in range(0, aug_times):\n",
        "                  mode_k=np.random.randint(0,8)\n",
        "                  patch_aug = data_augmentation(patch, mode=mode_k)\n",
        "                  clean_patch_aug = data_augmentation(clean_patch, mode=mode_k)\n",
        "                  patches.append(patch_aug)\n",
        "                  clean_patches.append(clean_patch_aug)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return clean_patches, patches"
      ],
      "metadata": {
        "id": "6ktmhpD-8xur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Data Augmentation'''\n",
        "def data_augmentation(img, mode=0):\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return np.flipud(img)\n",
        "    elif mode == 2:\n",
        "        return np.rot90(img)\n",
        "    elif mode == 3:\n",
        "        return np.flipud(np.rot90(img))\n",
        "    elif mode == 4:\n",
        "        return np.rot90(img, k=2)\n",
        "    elif mode == 5:\n",
        "        return np.flipud(np.rot90(img, k=2))\n",
        "    elif mode == 6:\n",
        "        return np.rot90(img, k=3)\n",
        "    elif mode == 7:\n",
        "        return np.flipud(np.rot90(img, k=3))"
      ],
      "metadata": {
        "id": "UAKj62Vp8bXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Show Images'''\n",
        "def show(x,title=None,cbar=False,figsize=None):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(x,interpolation='nearest',cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if cbar:\n",
        "        plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KHhB2Ggv70Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''Loss Function'''\n",
        "def sum_squared_error(y_true, y_pred):\n",
        "    #return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "    #return K.sum(K.square(y_pred - y_true), axis=-1)/2\n",
        "    return K.sum(K.square(y_pred - y_true))/2"
      ],
      "metadata": {
        "id": "uCsHTWNK78rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DIVA2D(depth, filters=64, image_channels=1, kernel_size=5, use_bnorm=True):\n",
        "    layer_count = 0\n",
        "    inpt = Input(shape=(None, None, image_channels), name='input'+str(layer_count))\n",
        "\n",
        "    # Get the initial patches\n",
        "    initial_patches = Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), strides=(1, 1),\n",
        "                             kernel_initializer='Orthogonal', padding='same', name='initial_patches')(inpt)\n",
        "    initial_patches = Activation('relu', name='initial_patch_acti')(initial_patches)\n",
        "\n",
        "    # Get interactions using attention\n",
        "    attention_inter = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=filters)(initial_patches, initial_patches)\n",
        "    attention_inter = LayerNormalization()(attention_inter + attention_inter)\n",
        "    attention_inter = Activation('relu', name='attn_acti')(attention_inter)\n",
        "\n",
        "    # Get contributions of the original potential in the Hamiltonian kernel\n",
        "    ori_poten_kernel = MaxPooling2D(pool_size=(21, 21), strides=(15, 15), padding='same', name='ori_poten_ker')(initial_patches)\n",
        "\n",
        "    # Get contributions of the interactions in the Hamiltonian kernel\n",
        "    inter_kernel = MaxPooling2D(pool_size=(21, 21), strides=(15, 15), padding='same', name='inter_ker')(attention_inter)\n",
        "\n",
        "    # Get projection coefficients of the initial patches on the Hamiltonian kernel\n",
        "    x = Hamiltonian_Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), kernel_3=ori_poten_kernel,\n",
        "                           kernel_4=inter_kernel, strides=(1, 1), activation='relu',\n",
        "                           kernel_initializer='Orthogonal', padding='same', name='proj_coef')(attention_inter)\n",
        "\n",
        "    # Thresholding\n",
        "    for i in range(depth):\n",
        "        layer_count += 1\n",
        "        x = Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), strides=(1, 1),\n",
        "                   kernel_initializer='Orthogonal', padding='same', use_bias=False, name='conv'+str(layer_count))(x)\n",
        "\n",
        "        layer_count += 1\n",
        "        x = BatchNormalization(axis=3, momentum=0.1, epsilon=0.0001, name='bn'+str(layer_count))(x)\n",
        "        x = Activation('relu', name='Thresholding'+str(layer_count))(x)\n",
        "\n",
        "    # Inverse projection\n",
        "    x = Conv2D(filters=image_channels, kernel_size=(kernel_size, kernel_size), strides=(1, 1),\n",
        "               kernel_initializer='Orthogonal', padding='same', use_bias=False, name='inv_trans')(x)\n",
        "\n",
        "    # Deconvolution layer\n",
        "    layer_count += 1\n",
        "    x = Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), strides=(1, 1),\n",
        "               kernel_initializer='Orthogonal', padding='same', use_bias=False, name='deconv'+str(layer_count))(x)\n",
        "    layer_count += 1\n",
        "    x = Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), strides=(1, 1),\n",
        "               kernel_initializer='Orthogonal', padding='same', use_bias=False, name='deconv'+str(layer_count))(x)\n",
        "    layer_count += 1\n",
        "    x = Conv2D(filters=image_channels, kernel_size=(kernel_size, kernel_size), strides=(1, 1),\n",
        "               kernel_initializer='Orthogonal', padding='same', use_bias=False, name='deconv'+str(layer_count))(x)\n",
        "    layer_count += 1\n",
        "\n",
        "    x = Subtract(name='subtract')([inpt, x])  # input - noise\n",
        "\n",
        "    model = Model(inputs=inpt, outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TJOxFp6R76eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Ajlpp5ueWM"
      },
      "outputs": [],
      "source": [
        " '''Hamiltonian convolution layer'''\n",
        "class Hamiltonian_Conv2D(Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, kernel_3=None, kernel_4=None, activation=None, use_bias = False, **kwargs):\n",
        "\n",
        "        self.rank = 2               # Dimension of the kernel\n",
        "        self.num_filters = filters  # Number of filter in the convolution layer\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank, 'kernel_size')\n",
        "        self.kernel_3 = kernel_3    # Weights from original potential\n",
        "        self.kernel_4 = kernel_4    # Weights from interaction\n",
        "\n",
        "        super(Hamiltonian_Conv2D, self).__init__(self.num_filters, self.kernel_size,\n",
        "              activation=activation, use_bias=False, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                     'should be defined. Found `None`.')\n",
        "\n",
        "        #don't use bias:\n",
        "        self.bias = None\n",
        "\n",
        "        #consider the layer built\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "        # Define nabla operator\n",
        "        weights_1 = tf.constant([[ 2.,-1., 0.],\n",
        "                                 [-1., 4.,-1.],\n",
        "                                 [ 0.,-1., 2.]])\n",
        "\n",
        "\n",
        "        weights_1 = tf.reshape(weights_1 , [3,3, 1])\n",
        "        weights_1 = tf.repeat(weights_1 , repeats=self.num_filters, axis=2)\n",
        "        #print('kernel shape of weights_1:',weights_1.get_shape())\n",
        "\n",
        "        # Define Weights for h^2/2m  (size should be same as the nabla operator)\n",
        "        weights_2 = self.add_weight(shape=weights_1.get_shape(),\n",
        "                                      initializer= 'Orthogonal',\n",
        "                                      name='kernel_h^2/2m',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        #print('kernel shape of weights_2:',weights_2.get_shape())\n",
        "\n",
        "\n",
        "        # Define the Hamiltonian kernel\n",
        "        self.kernel = weights_1*weights_2 + self.kernel_3 + self.kernel_4\n",
        "        #print('self.kernel',self.kernel.get_shape())\n",
        "\n",
        "        self.built = True\n",
        "        super(Hamiltonian_Conv2D, self).build(input_shape)\n",
        "\n",
        "    # Do the 2D convolution using the Hamiltonian kernel\n",
        "    def convolution_op(self, inputs, kernel):\n",
        "        if self.padding == \"causal\":\n",
        "            tf_padding = \"VALID\"  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, str):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "\n",
        "\n",
        "        return tf.nn.convolution(\n",
        "            inputs,\n",
        "            kernel,\n",
        "            strides=list(self.strides),\n",
        "            padding=tf_padding,\n",
        "            dilations=list(self.dilation_rate),\n",
        "            name=self.__class__.__name__,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.convolution_op(inputs, self.kernel)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DIVA2D(depth=15,filters=96,image_channels=1,use_bnorm=True)"
      ],
      "metadata": {
        "id": "oz0OteFv6AoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aNIHpeSg6KEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab9d4a6-b5ca-40b7-e326-6fffe031062f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input0 (InputLayer)         [(None, None, None, 1)]      0         []                            \n",
            "                                                                                                  \n",
            " initial_patches (Conv2D)    (None, None, None, 96)       2496      ['input0[0][0]']              \n",
            "                                                                                                  \n",
            " initial_patch_acti (Activa  (None, None, None, 96)       0         ['initial_patches[0][0]']     \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, None, None, 96)       37248     ['initial_patch_acti[0][0]',  \n",
            " iHeadAttention)                                                     'initial_patch_acti[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, None, None, 96)       0         ['multi_head_attention[0][0]',\n",
            " Lambda)                                                             'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, None, None, 96)       192       ['tf.__operators__.add[0][0]']\n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " attn_acti (Activation)      (None, None, None, 96)       0         ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " proj_coef (Hamiltonian_Con  (None, None, None, 96)       231264    ['attn_acti[0][0]']           \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, None, None, 96)       230400    ['proj_coef[0][0]']           \n",
            "                                                                                                  \n",
            " bn2 (BatchNormalization)    (None, None, None, 96)       384       ['conv1[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding2 (Activation)  (None, None, None, 96)       0         ['bn2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding2[0][0]']       \n",
            "                                                                                                  \n",
            " bn4 (BatchNormalization)    (None, None, None, 96)       384       ['conv3[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding4 (Activation)  (None, None, None, 96)       0         ['bn4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv5 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding4[0][0]']       \n",
            "                                                                                                  \n",
            " bn6 (BatchNormalization)    (None, None, None, 96)       384       ['conv5[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding6 (Activation)  (None, None, None, 96)       0         ['bn6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv7 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding6[0][0]']       \n",
            "                                                                                                  \n",
            " bn8 (BatchNormalization)    (None, None, None, 96)       384       ['conv7[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding8 (Activation)  (None, None, None, 96)       0         ['bn8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv9 (Conv2D)              (None, None, None, 96)       230400    ['Thresholding8[0][0]']       \n",
            "                                                                                                  \n",
            " bn10 (BatchNormalization)   (None, None, None, 96)       384       ['conv9[0][0]']               \n",
            "                                                                                                  \n",
            " Thresholding10 (Activation  (None, None, None, 96)       0         ['bn10[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv11 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding10[0][0]']      \n",
            "                                                                                                  \n",
            " bn12 (BatchNormalization)   (None, None, None, 96)       384       ['conv11[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding12 (Activation  (None, None, None, 96)       0         ['bn12[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv13 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding12[0][0]']      \n",
            "                                                                                                  \n",
            " bn14 (BatchNormalization)   (None, None, None, 96)       384       ['conv13[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding14 (Activation  (None, None, None, 96)       0         ['bn14[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv15 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding14[0][0]']      \n",
            "                                                                                                  \n",
            " bn16 (BatchNormalization)   (None, None, None, 96)       384       ['conv15[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding16 (Activation  (None, None, None, 96)       0         ['bn16[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv17 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding16[0][0]']      \n",
            "                                                                                                  \n",
            " bn18 (BatchNormalization)   (None, None, None, 96)       384       ['conv17[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding18 (Activation  (None, None, None, 96)       0         ['bn18[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv19 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding18[0][0]']      \n",
            "                                                                                                  \n",
            " bn20 (BatchNormalization)   (None, None, None, 96)       384       ['conv19[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding20 (Activation  (None, None, None, 96)       0         ['bn20[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv21 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding20[0][0]']      \n",
            "                                                                                                  \n",
            " bn22 (BatchNormalization)   (None, None, None, 96)       384       ['conv21[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding22 (Activation  (None, None, None, 96)       0         ['bn22[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv23 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding22[0][0]']      \n",
            "                                                                                                  \n",
            " bn24 (BatchNormalization)   (None, None, None, 96)       384       ['conv23[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding24 (Activation  (None, None, None, 96)       0         ['bn24[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv25 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding24[0][0]']      \n",
            "                                                                                                  \n",
            " bn26 (BatchNormalization)   (None, None, None, 96)       384       ['conv25[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding26 (Activation  (None, None, None, 96)       0         ['bn26[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv27 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding26[0][0]']      \n",
            "                                                                                                  \n",
            " bn28 (BatchNormalization)   (None, None, None, 96)       384       ['conv27[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding28 (Activation  (None, None, None, 96)       0         ['bn28[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv29 (Conv2D)             (None, None, None, 96)       230400    ['Thresholding28[0][0]']      \n",
            "                                                                                                  \n",
            " bn30 (BatchNormalization)   (None, None, None, 96)       384       ['conv29[0][0]']              \n",
            "                                                                                                  \n",
            " Thresholding30 (Activation  (None, None, None, 96)       0         ['bn30[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " inv_trans (Conv2D)          (None, None, None, 1)        2400      ['Thresholding30[0][0]']      \n",
            "                                                                                                  \n",
            " deconv31 (Conv2D)           (None, None, None, 96)       2400      ['inv_trans[0][0]']           \n",
            "                                                                                                  \n",
            " deconv32 (Conv2D)           (None, None, None, 96)       230400    ['deconv31[0][0]']            \n",
            "                                                                                                  \n",
            " deconv33 (Conv2D)           (None, None, None, 1)        2400      ['deconv32[0][0]']            \n",
            "                                                                                                  \n",
            " subtract (Subtract)         (None, None, None, 1)        0         ['input0[0][0]',              \n",
            "                                                                     'deconv33[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3970560 (15.15 MB)\n",
            "Trainable params: 3967680 (15.14 MB)\n",
            "Non-trainable params: 2880 (11.25 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Hyperparameters'''\n",
        "patch_size, stride = 25, 10\n",
        "aug_times = 1\n",
        "scales = [1] # [1, 0.9, 0.8, 0.7]\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "P5k_OuLm7o6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # model selection\n",
        "    model = DIVA2D(depth=15,filters=96,image_channels=1,use_bnorm=True)\n",
        "    #model.summary()\n",
        "\n",
        "    # load the last model in matconvnet style\n",
        "    initial_epoch = findLastCheckpoint(save_dir=save_dir)\n",
        "    if initial_epoch > 0:\n",
        "        print('resuming by loading epoch %03d'%initial_epoch)\n",
        "        model.load_weights(os.path.join(save_dir,'model_%03d.hdf5'%initial_epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=Adam(0.001), loss= tf.keras.losses.MeanSquaredError(), #tf.keras.losses.CosineSimilarity (axis=-1, reduction=\"auto\", name=\"cosine_similarity\"),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(),\n",
        "                           tf.keras.metrics.RootMeanSquaredError(),\n",
        "                           tf.keras.metrics.MeanSquaredLogarithmicError(),\n",
        "                           tf.keras.metrics.MeanAbsoluteError(),\n",
        "                           sum_squared_error])\n",
        "\n",
        "    # tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.CosineSimilarity(name=\"cosine_similarity\", dtype=None, axis=-1),\n",
        "    # tf.keras.metrics.LogCoshError(),\n",
        "\n",
        "    # use call back functions\n",
        "    checkpointer = ModelCheckpoint(os.path.join(save_dir,'model_{epoch:03d}.hdf5'),\n",
        "                verbose=1, save_weights_only=False, period=1)\n",
        "    csv_logger = CSVLogger(os.path.join(save_dir,'log.csv'), append=True, separator=',')\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    print('batch_size = ',args.batch_size)\n",
        "    history = model.fit(train_datagen(batch_size=args.batch_size),\n",
        "                steps_per_epoch=4750, epochs=60, verbose=1, initial_epoch=initial_epoch,\n",
        "                callbacks=[checkpointer,csv_logger,lr_scheduler])\n",
        "    #steps_per_epoch = 7000, epochs = 50"
      ],
      "metadata": {
        "id": "LxqNGvHd7-zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "8czxZmoNRcii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}